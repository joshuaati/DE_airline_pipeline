###
### A complete description of a Prefect Deployment for flow 'main-etl'
###
name: web_to_bucket
description: |-
  This function is the main entry point for the ETL (Extract, Transform, Load) pipeline that downloads a zip file from a specified URL,
  extracts it, and uploads the extracted files to a Google Cloud Storage bucket.

  Args:
      url : The URL of the zip file to download
      output_dir : The directory to save the downloaded and extracted files to.
      bucket_name : The name of the Google Cloud Storage bucket to upload the files to.

  Returns:
      None
version: d9530637c301b334fa944932de60bdbb
# The work queue that will handle this deployment's runs
work_queue_name: default
work_pool_name: null
tags: []
parameters:
  {
    "url": "https://dataverse.harvard.edu/api/access/dataset/:persistentId/?persistentId=doi:10.7910/DVN/HG7NV7",
    "output_dir": "data",
    "bucket_name": "airline-buckets",
  }
schedule: null
is_schedule_active: null
infra_overrides: {}
infrastructure:
  type: process
  env: {}
  labels: {}
  name: null
  command: null
  stream_output: true
  working_dir: null
  block_type_slug: process
  _block_type_slug: process

###
### DO NOT EDIT BELOW THIS LINE
###
flow_name: main-etl
manifest_path: null
storage: null
path: /workspaces/DE_airline_on_time
entrypoint: main/etl_web_gcp.py:main_etl
parameter_openapi_schema:
  title: Parameters
  type: object
  properties:
    url:
      title: url
      description: The URL of the zip file to download
      position: 0
      type: string
    output_dir:
      title: output_dir
      description: The directory to save the downloaded and extracted files to.
      position: 1
      type: string
    bucket_name:
      title: bucket_name
      description:
        The name of the Google Cloud Storage bucket to upload the files
        to.
      position: 2
      type: string
  required:
    - url
    - output_dir
    - bucket_name
  definitions: null
timestamp: "2023-03-24T00:08:50.295950+00:00"
